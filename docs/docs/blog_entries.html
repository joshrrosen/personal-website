<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-07-07">

<title>Romantically Involved with Statistics - Introduction to Bayesian Hierarchical Modeling in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Romantically Involved with Statistics</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../current_projects.html">
 <span class="menu-text">Current Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../cv.html">
 <span class="menu-text">Curriculumn Vitae</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../even_more.html">
 <span class="menu-text">Other ‘Projects’</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog_entries.html">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#welcome" id="toc-welcome" class="nav-link active" data-scroll-target="#welcome">Welcome!</a></li>
  <li><a href="#section-1-lets-get-bay-curious" id="toc-section-1-lets-get-bay-curious" class="nav-link" data-scroll-target="#section-1-lets-get-bay-curious">Section 1: Let’s Get Bay-Curious</a></li>
  <li><a href="#section-2-introduction-to-bayesian-hierarchical-modeling" id="toc-section-2-introduction-to-bayesian-hierarchical-modeling" class="nav-link" data-scroll-target="#section-2-introduction-to-bayesian-hierarchical-modeling">Section 2: Introduction to Bayesian Hierarchical Modeling</a>
  <ul class="collapse">
  <li><a href="#motivating-example-air-quality-withinacross-cities" id="toc-motivating-example-air-quality-withinacross-cities" class="nav-link" data-scroll-target="#motivating-example-air-quality-withinacross-cities">Motivating Example: Air Quality Within/Across Cities</a>
  <ul class="collapse">
  <li><a href="#estimate-the-city-level-mean-for-each-city-using-only-data-from-that-city" id="toc-estimate-the-city-level-mean-for-each-city-using-only-data-from-that-city" class="nav-link" data-scroll-target="#estimate-the-city-level-mean-for-each-city-using-only-data-from-that-city">Estimate the city-level mean for each city, using only data from that city</a></li>
  <li><a href="#estimate-each-city-using-all-data-in-the-state" id="toc-estimate-each-city-using-all-data-in-the-state" class="nav-link" data-scroll-target="#estimate-each-city-using-all-data-in-the-state">Estimate each city using all data in the state</a></li>
  <li><a href="#estimate-the-mean-with-a-hierarchical-model-using-partial-pooling" id="toc-estimate-the-mean-with-a-hierarchical-model-using-partial-pooling" class="nav-link" data-scroll-target="#estimate-the-mean-with-a-hierarchical-model-using-partial-pooling">Estimate the mean with a hierarchical model using partial pooling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#section-3-brms-and-applications-of-hierarchical-modeling-in-stan" id="toc-section-3-brms-and-applications-of-hierarchical-modeling-in-stan" class="nav-link" data-scroll-target="#section-3-brms-and-applications-of-hierarchical-modeling-in-stan">Section 3: ‘brms()’ and Applications of Hierarchical Modeling in Stan</a>
  <ul class="collapse">
  <li><a href="#review-of-brms-and-mcmc-sampling" id="toc-review-of-brms-and-mcmc-sampling" class="nav-link" data-scroll-target="#review-of-brms-and-mcmc-sampling">Review of ‘brms()’ and MCMC sampling</a></li>
  <li><a href="#hierarchical-modeling-in-brms" id="toc-hierarchical-modeling-in-brms" class="nav-link" data-scroll-target="#hierarchical-modeling-in-brms">Hierarchical Modeling in ‘brms()’</a></li>
  </ul></li>
  <li><a href="#summary-and-conclusion" id="toc-summary-and-conclusion" class="nav-link" data-scroll-target="#summary-and-conclusion">Summary and Conclusion</a></li>
  <li><a href="#miscellaneous-citations" id="toc-miscellaneous-citations" class="nav-link" data-scroll-target="#miscellaneous-citations">Miscellaneous Citations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Bayesian Hierarchical Modeling in R</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 7, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell">

</div>
<section id="welcome" class="level1">
<h1>Welcome!</h1>
<p>Welcome to part 1 of N on Bayesian Hierarchical Modeling in R. I intend for this to be a series where we can explore in evermore detail both the fundamentals and applications of Bayesian hierarchical modeling. The goal: as <span class="math inline">\(lim_{{N \to \infty}} f(N)\)</span>, we’ll find ourselves more equipped to consider and apply these methods to a broad array of potential problems.</p>
<p>In the limit, we will all be Bayesians.</p>
<p>The intended audience is someone with a sound understanding of statistics, in either the Bayesian or Frequentist camps. That being said, don’t expect to find anything too technical in this article (no asymptotics here). I’m targeting this blog post for students seeking to both expand their skill set/intuition and get a little little more out of their modeling approach. In addition, I will provide a brief review of Bayesian estimation for those coming in with more of a Frequentist background.</p>
<p>In terms of programming experience, I expect readers to be well versed in R. However, I will introduce new concepts and tools as they become relevant. The majority of this tutorial will utilize the <a href="https://cran.r-project.org/web/packages/brms/index.html">‘brms()’</a> package in R. To the extent that this blog post even touches RStan, I will be sure to define and explain anything that looks a bit too <em>foreign</em> to the average R user.</p>
<p>So with that, let’s rock and roll.</p>
</section>
<section id="section-1-lets-get-bay-curious" class="level1">
<h1>Section 1: Let’s Get Bay-Curious</h1>
<p>In this section I’ll provide a brief but sufficient review on the fundamentals of Bayesian inference. Readers who are familiar with the Bayesian framework (e.g., the philosophical assumptions and Bayes’ Theorem) can feel free to skip right on ahead to the next section.</p>
<p>Let’s first recall that Bayesian inference allows us to estimate a <strong>posterior distribution</strong> <span class="math inline">\(p(\mu | y)\)</span> as the product of our <strong>prior</strong> knowledge <span class="math inline">\(p(\mu)\)</span> and some <strong>likelihood function</strong> <span class="math inline">\(p(\mathbf{y} | \mu)\)</span>. Here the prior distribution <span class="math inline">\(p(\mu)\)</span> reflects any knowledge about the parameter of interest <span class="math inline">\(\mu\)</span> that we wish to incorporate. Likewise, the likelihood function (sometimes referred to as the sampling distribution) specifies the relationship between the data <span class="math inline">\(\mathbf{y} = (y_1, ... , y_n)\)</span> and <span class="math inline">\(\mu\)</span>. This framework allows us to construct a posterior distribution by conditioning prior information on the data.</p>
<p>Using Bayes’ rule, we can formalize this process as:</p>
<p><span class="math display">\[p(\mu | \mathbf{y}) = \frac{ p(\mu) p(\mathbf{y} | \mu) }{ p(\mathbf{y}) }\]</span></p>
<p>In practice, we can drop the denominator and instead only consider terms that contain our parameter <span class="math inline">\(\mu\)</span>. Note that <span class="math inline">\(p(\mu | \mathbf{y})\)</span> is a probability density function with <span class="math inline">\(\int p(\mu | \mathbf{y}) d\mu = 1\)</span>, and what matters is how <span class="math inline">\(p(\mu | \mathbf{y})\)</span> depends on <span class="math inline">\(\mu\)</span>. Clearly <span class="math inline">\(p(\mathbf{y})\)</span> does not very with <span class="math inline">\(\mu\)</span>, so this term in the denominator only functions as scaling factor. As a result, it’s potentially easier to formulate the posterior approximately as: <span class="math display">\[p(\mu | \mathbf{y}) \propto  p(\mu)p(\mathbf{y} | \mu)\]</span> or: <span class="math display">\[ posterior \propto prior \times likelihood\]</span></p>
<p>Without losing the thread, I want to note again how wildly useful this framework of obtaining posterior distributions for our parameter of interest is. Using Bayes’ rule, we obtain a full probability model for our parameter of interest (in this case, <span class="math inline">\(\mu\)</span>), which is incredibly useful for conducting inference, and one of the primary actionable benefits that one gains when working in the Bayesian framework. Since we obtain a full probability model, we can summarize any information that we might need. I’m actually of the view that going full Bayesian does <strong>not yet</strong> require us to view probability under a new philosophical light<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> (I see it more as a way to be explicit about modeling assumptions and to incorporate additional uncertainty), but these small tweaks to the <em>world view</em> can be a helpful way to ease the transition. As you continue on your Bayesian journey (as we all must), it will soon make sense how contrasting views of probability can derive these consequential differences in our modeling approach.</p>
</section>
<section id="section-2-introduction-to-bayesian-hierarchical-modeling" class="level1">
<h1>Section 2: Introduction to Bayesian Hierarchical Modeling</h1>
<p>With a brief review of Bayes’ Theorem out of the way, we are now free to jump into the meat of this series: Bayesian hierarchical modeling. Hierarchical models, also referred to as multilevel models, are models in which parameters exist in a setting where there is a hierarchy of nested populations. In the simplest form, we can imagine data in which there are two levels: groups and units within groups. Using this approach, we allow for the analysis of complex data structures with multiple levels of variation.</p>
<p>Using this framework allows us to conduct inference on data where the observations are not identically and independently distributed (i.i.d.). For example, suppose we have a fair coin, and are interested in observing the probability of landing a heads. In this case, the observations (coin flips) are i.i.d. because each flip is independent of the others. When we flip the coin twice, the second flip does not depend on the first: thus each flip is independent. Similarly, each flip follows the same probability distribution, and we observe heads with uniform probability of <span class="math inline">\(p(H) = .5\)</span>.</p>
<p>To contrast this, let’s imagine a group of students each from different schools. Each school requires their own coin that is biased in some unknown way. Here, while individual flips remain independent, the underlying probability with which we will observe heads varies across schools. To account for this hierarchical structure, we can use hierarchical modeling.</p>
<section id="motivating-example-air-quality-withinacross-cities" class="level2">
<h2 class="anchored" data-anchor-id="motivating-example-air-quality-withinacross-cities">Motivating Example: Air Quality Within/Across Cities</h2>
<p>In the next section, we’ll work with an example examining air quality within cities. For now, let’s introduce this problem conceptually.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="blog_entries_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<ul>
<li>Data: AQI measurements by air monitors in cities in California.</li>
<li>Hierarchy: Monitors observed in cities, cities observed within a single state.</li>
<li>Question: How can we estimate the expected AQI for each city?</li>
</ul>
<p>Let’s also introduce a bit of notation just so nothing gets lost in translation. Here we aim to model our outcome variable <span class="math inline">\(y\)</span> (AQI) for monitor <span class="math inline">\(i = 1, 2, 3, ..., n\)</span>. Each monitor <span class="math inline">\(i\)</span> sits within a group <span class="math inline">\(j\)</span>, which here refers to cities. Therefore, index <span class="math inline">\(j[i]\)</span> denotes the monitor <span class="math inline">\(i\)</span> for city <span class="math inline">\([j]\)</span>.</p>
<p>If we wish to estimate the expected AQI for each city, there are multiple ways in which we might do so. As we will discuss, each option comes with its own positives and negatives.</p>
<ol type="1">
<li><p>Estimate the city-level mean for each city, using only data from the city.</p></li>
<li><p>Estimate each city using all data in the state.</p></li>
<li><p>Estimate the mean with a hierarchical model using partial pooling.</p></li>
</ol>
<section id="estimate-the-city-level-mean-for-each-city-using-only-data-from-that-city" class="level3">
<h3 class="anchored" data-anchor-id="estimate-the-city-level-mean-for-each-city-using-only-data-from-that-city">Estimate the city-level mean for each city, using only data from that city</h3>
<p>In this approach, we model the city-level mean as: <span class="math inline">\(\mu_i = \alpha_{j[i]}^{no \ pool}\)</span>, where for each city <span class="math inline">\(j\)</span>, we estimate <span class="math inline">\(\alpha_j^{no \ pool}\)</span> using only data from city <span class="math inline">\(j\)</span>. As a result, we see that <span class="math inline">\(\hat{\alpha}_j^{no \ pool} \approx \bar{y}_j\)</span>, the city sample mean.</p>
<p>We refer to this estimate as the <strong>no pooling model</strong>, since there is no pooling of information across cities. Intuitively, it’s easy to see the draw backs of this approach. For example, say the sample size varies across city, and we have reason to believe some city <span class="math inline">\(j\)</span> in particular lacks enough data to make a true, unbiased estimate of the city’s mean AQI. In this scenario, the no pooling option may lead us to poor estimates based on limited information.</p>
<p>Figure 2 visualizes this problem using simulated data. While this approach isn’t necessarily a bad one, it forces us to think constructively about the validity of our sampling approach and question if <span class="math inline">\(\hat{y}_j\)</span> is truly a strong estimate of <span class="math inline">\(y_j\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="blog_entries_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="estimate-each-city-using-all-data-in-the-state" class="level3">
<h3 class="anchored" data-anchor-id="estimate-each-city-using-all-data-in-the-state">Estimate each city using all data in the state</h3>
<p>Moving on to our second strategy, we will attempt to estimate each city mean using all data from the state. In this model, <span class="math inline">\(\mu_i = \mu\)</span>, the state-level mean. In contrast to the no pooling options demonstrated in the previous section, this approach is known as <strong>complete pooling</strong>, since we completely pool information across cities. In other words, we ignore one level of hierarchy (here, cities) and estimate the mean as if our data was taken from a single, state-level sample.</p>
<p>Just as with no pooling, it doesn’t take long to theorize potential problems with this approach. For one, when we completely pool information between cities, we ignore across-city variance which may result in inaccurate estimates for cities with outlying levels. In Figure 2, we denote the across-city mean with a red dotted line, and can easily see that using this estimate would drop significant between-city variation in AQI.</p>
</section>
<section id="estimate-the-mean-with-a-hierarchical-model-using-partial-pooling" class="level3">
<h3 class="anchored" data-anchor-id="estimate-the-mean-with-a-hierarchical-model-using-partial-pooling">Estimate the mean with a hierarchical model using partial pooling</h3>
<p>Finally, we arrive at our last option for estimating mean AQI. In this approach, we formulate a hierarchical model for estimating city-level AQI as follows: <span class="math display">\[y_i | \alpha_{j[i]}, \sigma_y \sim N(\alpha_{j[i]}, \sigma_y^2)\]</span>, <span class="math display">\[\alpha_j | \mu_{\alpha}, \sigma_{\alpha}^2 \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\alpha_j\)</span> represents the city-specific mean,</p></li>
<li><p><span class="math inline">\(\mu_{\alpha}\)</span> represents the mean of the city AQI levels,</p></li>
<li><p><span class="math inline">\(\sigma_{\alpha}^2\)</span> represents the between-city variance</p></li>
</ul>
<p>Due to this hierarchical set-up, our estimates for city mean AQI will be between the estimates derived from the no pooling and complete pooling methods presented earlier. Typically, we say that the <strong>partially pooled</strong> mean is shrunk from the no pooling city sample mean towards the complete pooling state mean. This part might not be super intuitive, but we’ll break it down a bit further. First, we can observe that the extent of shrinkage<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> from the city-level sample mean towards the state mean <em>decreases</em> with sample size. In other words, as we observe more data at the city-level, the partially-pooled mean is <em>more</em> reliant on that city-level data. Put another way, when we observe enough data from a city to be confident in our sample mean, we become <em>less</em> reliant on the broader state-level mean.</p>
<p>To formalize this, let’s observe the conditional distribution <span class="math inline">\(\alpha_j | \mathbf{y}, \mu_{\alpha}, \sigma_y, \sigma_{\alpha}\)</span>. For the multilevel model <span class="math display">\[y_i | \alpha_j[i], \sigma_y \sim N(\alpha{j[i]}, \sigma_y^2)\]</span> with <span class="math display">\[\alpha_j | \mu_{\alpha}, \sigma_{\alpha}^2 \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)\]</span> the conditional distribution for the <span class="math inline">\(j\)</span>-th state mean is given by: <span class="math display">\[\alpha_j | \mathbf{y}, \mu_{\alpha}, \sigma_y, \sigma_{\alpha} \sim N(m,v),\]</span> <span class="math display">\[v = (n_j/\sigma_y^2 + 1/\sigma_{\alpha}^2)^{-1},\]</span> <span class="math display">\[m = v \times (\frac{n_j}{\sigma_y^2}\times \bar{y}_j + \frac{1}{\sigma_{\alpha}^2}\mu_{\alpha}) \Rightarrow \frac{(\frac{n_j}{\sigma_y^2}\times \bar{y}_j + \frac{1}{\sigma_{\alpha}^2}\mu_{\alpha})}{(n_j/\sigma_y^2 + 1/\sigma_{\alpha}^2)},\]</span> where <span class="math inline">\(n_j\)</span> is the number of observations (in this case, monitors) in city <span class="math inline">\(j\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Given this, <span class="math inline">\(E[\alpha_j]\)</span> is given by <span class="math inline">\(m_j = w_j\bar{y}_j + (1 - w_j) \mu_{\alpha}\)</span>, the weighted average of city sample mean <span class="math inline">\(\bar{y}_j\)</span> and the mean of city-level means <span class="math inline">\(\mu_{\alpha}\)</span>. Notably, the weight <span class="math inline">\(w_j\)</span> is defined by <span class="math display">\[w_j = \frac{n_j}{\sigma_y^2} \times (n_j/\sigma_y^2 + 1/\sigma_{\alpha}^2)^{-1},\]</span> which increases with <span class="math inline">\(n_j\)</span> (the number of observations, in this case monitors, in city <span class="math inline">\(j\)</span>). While a bit long-winded, we can now see formally why the partially pooled estimate of <span class="math inline">\(alpha_j\)</span> is shrunk from the city mean towards the state mean for cities with small sample sizes.</p>
<p>Next, we can visualize this process to continue understanding the intuition behind partial pooling. Figure 3 plots the difference between <span class="math inline">\(\alpha_j\)</span> (the partially pooled mean of city <span class="math inline">\(j\)</span>) and <span class="math inline">\(\bar{y}_j\)</span> (the sample mean for city <span class="math inline">\(j\)</span>) on the y-axis and the number of monitors by city (the number of observations) on the x-axis. In doing so, we see that as the number of monitors increases (as the number of <span class="math inline">\(n\)</span> observations increase), the difference between our two mean estimates shrinks towards zero. In other words, here we intuitively visualize that as the number of observations in a given city increases, our estimated mean grows closer to the city sample mean.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="blog_entries_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>For one final note, bear in mind that in order to estimate a full Bayesian hierarchical model of the stated form, we’ll need to establish priors on all model parameters: <span class="math inline">\(\sigma_y, \ \mu_{\alpha}, \ \sigma_{\alpha}\)</span>. Lucky for us, we’ll show in the next section that ‘brms()’ can be a great tool for providing default priors.</p>
</section>
</section>
</section>
<section id="section-3-brms-and-applications-of-hierarchical-modeling-in-stan" class="level1">
<h1>Section 3: ‘brms()’ and Applications of Hierarchical Modeling in Stan</h1>
<p>In this section, we’ll apply our knowledge of hierarchical models to a simple motivating example. Here we’ll construct a two-level hierarchical model for estimating group means using a normal distribution. To do this, we’ll again utilize the simulated data introduced earlier. Here is the code to reproduce this data on your own. For the non-Bayesians among us, I’ve included a brief review of ‘brms()’ and MCMC sampling that those with sufficient experience can feel free to skip.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the number of cities</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>num_cities <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the mean AQI values for each city</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>city_means <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">50</span>, <span class="dv">250</span>, <span class="at">length.out =</span> num_cities)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an empty data frame to store the simulated data</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through each city</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_cities) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Define the number of observations per city (varying significantly)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  num_obs <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">50</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simulate AQI values for the city using normal distribution</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  city_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">aqi =</span> <span class="fu">rnorm</span>(num_obs, <span class="at">mean =</span> city_means[i], <span class="at">sd =</span> <span class="dv">10</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Add a city identifier variable</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  city_data<span class="sc">$</span>city <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"City"</span>, i)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Append the city data to the overall dataset</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  dat <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(dat, city_data)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="review-of-brms-and-mcmc-sampling" class="level2">
<h2 class="anchored" data-anchor-id="review-of-brms-and-mcmc-sampling">Review of ‘brms()’ and MCMC sampling</h2>
<p>Before we dive in, let’s run through a brief description and review of the ‘brms()’ package in R. The ‘brms()’ package allows us to estimate multilevel models in Stan through our familiar R interface. Stan provides an MCMC sampler for Bayesian analysis by implementing a Hamiltonian Monte Carlo algorithm that converges comparatively quickly even in cases of high-dimensional models and non-conjugate priors.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> If you’re thinking <em>wow, that sounds cool - I wonder how that works?</em> you’re just gonna have to keep wondering because I also have no clue. If I tried to explain, I’d just get it wrong and then whoops no one knows anything anymore. But man, what a cool bunch of words!</p>
<p>What you need to know is this: Bayesian stats is incredibly computationally intensive, and only through advances in modern computation are we able to do it. Posterior distributions, which are so very useful for inference, often can’t be solved analytically. Moreover, common choices of priors often do not result in closed form expressions. The good news is as long as we can obtain a sample from the distribution of interest, we can conduct inference. This is where <strong>Markov Chain Monte Carlo (MCMC)</strong> sampling comes in. Let’s say our goal is to estimate two parameters <span class="math inline">\((\mu, \sigma)\)</span> when <span class="math inline">\(y_i | \mu, \sigma \sim N(\mu, \sigma^2)\)</span> but for common choices of priors on <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> there is no closed form solution for the posterior distribution <span class="math inline">\(p(\mu | \mathbf{y})\)</span>. If we identify a target parameter vector <span class="math inline">\(\phi = (\mu , \sigma)\)</span>, MCMC allows us to obtain samples <span class="math inline">\(\phi^{(s)}\)</span> from the target distribution <span class="math inline">\(p(\phi | \mathbf{y})\)</span>. Under the Markov property, <span class="math inline">\(\phi^{(s)}\)</span> depends on <span class="math inline">\(\phi^{(s-1)}, \phi^{(s-2)}, ... , \phi^{(1)}\)</span> only though <span class="math inline">\(\phi^{(s-1)}\)</span>. We then approximate quantities of interest, for example <span class="math inline">\(E(\mu | \mathbf{y})\)</span>, using the resulting samples - hence the additional MC for Monte Carlo.</p>
<p>So in short, Stan provides a (relatively) computationally efficient method for obtaining posterior samples, and ‘brms()’ provides a more intuitive method of interacting with simple Stan models. If there is sufficient interest, I’d be happy to do a follow up series on programming in Stan and MCMC sampling.</p>
<p>With that background out of the way, let’s install and load the ‘brms()’ package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages("brms")</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The ‘brms()’ package functions the same as our simple lm() function - albiet for a few additional parameters. We still use the general forula <span class="math inline">\(y \sim 1\)</span> to estimate <span class="math inline">\(\mu\)</span> in <span class="math inline">\(y_i | \mu, \sigma \sim N(\mu, \sigma)\)</span>. If we choose not to specify priors (wow, not very Bayesian of us), ‘brms()’ provides defaults. For now, let’s fit a simple model to estimate the mean of a subset of our simulated (read: fake) AQI data. Note: my code will look a bit different from yours if you’re following along. This is because even simple Bayesian models can take a while to run, so I’ve included a statement telling R to only run the model if it has not been run before. If this does not apply to you, feel free to use only the bits under ‘# fit the model.’ Furthermore, since we are starting simple, this model does not include many of the additional arguments ‘brms()’ allows us to specify. As we move forward, we will begin including more to improve the efficiency of this model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load necessary packages</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># subset simulated AQI data to a single city</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>city1 <span class="ot">&lt;-</span> dat <span class="sc">%&gt;%</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">subset</span>(city <span class="sc">==</span> <span class="st">"City1"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the saved model exists</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">file.exists</span>(<span class="st">"saved_model2.rds"</span>)) {</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Load the saved model</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"saved_model.rds"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the model</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">brm</span>(aqi <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> city1)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Save the fitted model</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">saveRDS</span>(fit, <span class="st">"saved_model2.rds"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 4.6e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.039 seconds (Warm-up)
Chain 1:                0.024 seconds (Sampling)
Chain 1:                0.063 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 9e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.034 seconds (Warm-up)
Chain 2:                0.024 seconds (Sampling)
Chain 2:                0.058 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 9e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.036 seconds (Warm-up)
Chain 3:                0.034 seconds (Sampling)
Chain 3:                0.07 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 1.4e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.038 seconds (Warm-up)
Chain 4:                0.033 seconds (Sampling)
Chain 4:                0.071 seconds (Total)
Chain 4: </code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: aqi ~ 1 
   Data: city1 (Number of observations: 46) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept    49.80      1.51    46.85    52.72 1.00     3211     2268

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma    10.62      1.13     8.72    13.12 1.00     2589     2639

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<p>Depending on your familiarity with fitting Bayesian models, you’re probabl thinking two things. First, <em>wow that did that take so long!</em> And yes! You’d be right. Even with a tiny sample size of <span class="math inline">\(n=46\)</span>, this model takes much longer than far more complex Frequentist models. Second <em>what the f#&amp;k is all this output??</em> Unfortunately, you’ll have to hold off on an answer to the latter for a bit.</p>
<p>Based on the model results, we find that: <span class="math inline">\(\hat{\mu} = 49.80\)</span>, with a 95% credible interval of <span class="math inline">\((46.85, 52.72)\)</span>, and <span class="math inline">\(\hat{\sigma} = 10.62\)</span>. Also notice that we also obtain a 95% credible interval for <span class="math inline">\(\hat{\sigma}\)</span> of <span class="math inline">\((8.72, 13.12)\)</span>.</p>
<p>Congratulations! You’ve fit a simple Bayesian model. You’re addicted, I know it. Let’s go deeper.</p>
<p>Next we need to note the model diagnostics. Recall that Stan obtains posterior samples using MCMC. However, what happens if the samples are not independent draws from the target distribution? We can only use samples from an MCMC algorithm to do inference if we have allowed for the samples to be representative draws from our target distribution. As noted before, the MCMC algorithm generates a sequence of samples that approximate the posterior distribution of the model parameters. Sampling diagnostics thus help assess the convergence and mixing properties of the chains to verify the quality of the samples.</p>
<p>One important aspect of MCMC sampling diagnostics is chain convergence. Convergence refers to the property that the chains have explored the posterior distribution sufficiently and have reached a stationary state where further sampling does not significantly affect the results. In other words, the chains have “mixed” well and are sampling from the target distribution. If the chains have not converged, the samples may not accurately represent the posterior distribution, and any inferences drawn from them could be biased or misleading.</p>
<p>To assess chain convergence, various diagnostic methods can be employed. One commonly used approach is based on the examination of trace plots. A trace plot displays the values of a parameter across iterations (samples) of the MCMC algorithm for each chain. By visually inspecting the trace plots, one can look for indications of convergence, such as stable behavior, absence of large trends, and absence of excessive autocorrelation (i.e., dependence between consecutive samples). If the trace plots exhibit erratic or unstable patterns, it suggests that further sampling may be required to achieve convergence. Figure 4 presents the trace plot for our earlier Bayesian model. This plot shows the sampled parameter against the iteration number. Notice in both Figure 4 and our model output an initial phase of the MCMC chain, when the chain converges away from initial values towards the target distribution. This is called the <strong>burn-in phase</strong> and allows us to discard the autocorrelated period when <span class="math inline">\(s+1\)</span> depends on <span class="math inline">\(s\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">variable =</span> <span class="fu">c</span>(<span class="st">"b_Intercept"</span>, <span class="st">"sigma"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="blog_entries_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can also use ‘brms()’ to visualize the full posterior distribution for our target parameters. Figure 4 thus includes both the trace plots and posterior densities for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Using these plots, we see evidence that our chains are mixing well and autocorrelation in the sampled values is low.</p>
<p>Aside from mixing, we still need to ensure that we’ve generated an appropriate number of samples. To do this, we rely on two new diagnostic criteria: <span class="math inline">\(\hat{R}\)</span> and effective sample size. First, The Gelman-Rubin convergence diagnostic statistic <span class="math inline">\(\hat{R}\)</span> compares the within-chain and between-chain variances to assess convergence.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> When chains are mixed well the <span class="math inline">\(\hat{R}\)</span> is close to <span class="math inline">\(1\)</span>. Values greater than <span class="math inline">\(1\)</span> suggest lack of convergence. Lucky for us, ‘brms()’ automatically computes <span class="math inline">\(\hat{R}\)</span> for each parameter. A good rule of thumb is to run at least four chains (the more the better!) with disperse starting points and to aim for <span class="math inline">\(\hat{R} &lt; 1.05\)</span>. As a result, we see that our model output suggests the chains have converged on both parameters.</p>
<p>Next, let’s look at effective sample size <span class="math inline">\(S_{eff}\)</span>. Here, effective sample size gives us the number of independent MC samples that would give the same precision for estimating the mean, as obtained with the MCMC sample of size <span class="math inline">\(S\)</span>. Put simply, effective sample size quantifies the effective number of independent samples obtained from the MCMC chains. If the chains are highly autocorrelated, the effective sample size will be smaller than the actual sample size.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> For example, if our sample size <span class="math inline">\(S = 2000\)</span>, but <span class="math inline">\(S_{eff} \approx 11\)</span>, then our chains are autocorrelated. The ‘brms()’ output reports both bulk-ESS and tail-ESS, which measure the effective sample size for mean and median estimates, and the minimum effective sample size for <span class="math inline">\(5 \%\)</span> and <span class="math inline">\(95 \%\)</span> quantiles, respectively. Per Markov Chain, we want bulk-ESS and tail-ESS to be <span class="math inline">\(\ge 100\)</span>. If there are issues presented by either <span class="math inline">\(S_{eff}\)</span> or <span class="math inline">\(\hat{R}\)</span>, we know something is wrong. That being said, the magnitude of the problem can vary, and we can often increase the number of iterations to increase the warm-up and resolve the issue. In other cases, it may be necessary to reparametrize and tune the MCMC parameters.</p>
<p>Now that we’ve introduced a few more diagnostics, we can return to our ‘brms()’ function to investigate a few more arguments. Below I’ve included a list explaining each argument in sufficient detail.</p>
<ul>
<li><p>‘chains = 4’: This argument determines the number of MCMC chains to run. In this example, the model will use four parallel chains. Running multiple chains helps assess chain convergence and improves the stability of the sampling process.</p></li>
<li><p>‘iter = 1000’: The ‘iter’ argument defines the total number of iterations per chain. In this case, each chain will run for 1000 iterations in total. These iterations include both warmup and sampling iterations.</p></li>
<li><p>‘warmup = 500’: The ‘warmup’ argument specifies the number of warmup iterations per chain. During warmup, the MCMC algorithm adapts its parameters to the posterior distribution and explores the parameter space. These warmup iterations are typically discarded when computing the posterior summaries. Here, each chain will have 500 warmup iterations.</p></li>
<li><p>‘cores = getOption(“mc.cores”, 4)’: The ‘cores’ argument determines the number of CPU cores to be used for parallel computation. In this example, the ‘getOption(“mc.cores”, 4)’ part allows the model to automatically detect the available number of CPU cores. It uses either the number of cores specified by the user or defaults to 4 cores if not specified.</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the saved model exists</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">file.exists</span>(<span class="st">"saved_model2.rds"</span>)) {</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Load the saved model</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"saved_model.rds"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the model</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">brm</span>(aqi <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> city1, </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">1000</span>, <span class="at">warmup =</span> <span class="dv">500</span>, <span class="at">cores =</span> <span class="fu">getOption</span>(<span class="st">"mc.cores"</span>, <span class="dv">4</span>))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Save the fitted model</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">saveRDS</span>(fit, <span class="st">"saved_model2.rds"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="hierarchical-modeling-in-brms" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-modeling-in-brms">Hierarchical Modeling in ‘brms()’</h2>
<p>And just like that we’ve reached the climax of our lesson: fitting hierarchical models in ‘brms()’. Are you amped? I know I am.</p>
<p>Let’s return once again to our simulated AQI example. Just as before, our goal is to estimate the expected AQI in each city. Fitting the model hierarchically allows us to generate partially-pooled estimates, where our within-city means are shrunk towards the state-level mean as the city sample size decreases. It’s sorta like magic, and if it makes you feel ~a certain type of way~ I wouldn’t blame you.</p>
<p>For this simplified model with no additional predictors, it’s fairly straightforward to estimate in ‘brms()’. To do so, we add “| grouping variable” to any covariate (in this case, the intercept) for which we aim to estimate group-specific parameters. The ‘brms()’ package also provides output on group-level effects (<span class="math inline">\(\hat{\sigma}_n = 60.14\)</span>), population-level effects (<span class="math inline">\(\hat{\mu}_{\alpha} =151.16\)</span>), and family-specific parameters (<span class="math inline">\(\hat{\sigma}_y = 10.02\)</span>). We are also blessed to be given the number of levels (here <span class="math inline">\(50\)</span>) which can serve as a fail safe to ensure we haven’t made some glaring mistake. Finally, notice that I’ve bumped up the chains and iterations as an easy tweak to improve convergence. As a fun exercise, try messing with these arguments and glance at the <span class="math inline">\(\hat{R}\)</span> and trace plots to see an example of what not to do.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the saved model exists</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">file.exists</span>(<span class="st">"saved_model.rds"</span>)) {</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Load the saved model</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">"saved_model.rds"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Fit the model (note the addition of the grouping variable)</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">&lt;-</span> <span class="fu">brm</span>(aqi <span class="sc">~</span> (<span class="dv">1</span><span class="sc">|</span>city),</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> dat,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">iter =</span> <span class="dv">3000</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">chains =</span> <span class="dv">4</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">cores =</span> <span class="fu">getOption</span>(<span class="st">"mc.cores"</span>, <span class="dv">4</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Save the fitted model</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">saveRDS</span>(fit2, <span class="st">"saved_model.rds"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: aqi ~ (1 | city) 
   Data: dat (Number of observations: 2408) 
  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;
         total post-warmup draws = 2000

Group-Level Effects: 
~city (Number of levels: 50) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)    60.34      5.45    51.90    72.44 1.13       23      165

Population-Level Effects: 
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept   148.16      7.72   135.24   165.97 1.33       10       41

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma    10.01      0.14     9.73    10.30 1.03      194      497

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<p>Finally, I’ll leave it as an exercise for the reader to discover how to visualize the parameter densities and trace plots for each estimated parameter. Unlike before, we have more than just two parameters. Regardless, ‘brms()’ makes it easy to access the entire sweet of information provided by the full posterior density.</p>
</section>
</section>
<section id="summary-and-conclusion" class="level1">
<h1>Summary and Conclusion</h1>
<p>In summary, today we took our first steps into the world Bayesian hierarchical modeling. And what an exciting world it is… Go out, tell your friends what you saw. They won’t believe you - but they’re cowards. You are brave, brave and a hero. You are a Bayesian.</p>
<p>More importantly, we showed how this framework provides a valuable way to estimate complex relationships and make informed decisions based on hierarchical data. By embracing the power of Bayesian inference, we can unlock new levels of flexibility and accuracy in our statistical modeling processes. Bayesian hierarchical modeling empowers researchers and analysts to incorporate prior knowledge, handle uncertainty, and address heterogeneity within their data, ultimately leading to more robust and reliable results.</p>
<p>In the next part, we’ll go a bit deeper and investigate how we can use the framework of hierarchical modeling to predict a non-sampled unit at both the city- and monitor-level, as well as how we can include unit-level predictors. We’ll also spend some time on the difference between fixed and random effects (hint: it depends) and discussing how multilevel models fit within the Bayesian framework. Finally, part two will cover how we can set explicit priors in ‘brms()’, and how we can tackle all of this directly in Stan. Buckle up hoes.</p>
</section>
<section id="miscellaneous-citations" class="level1">
<h1>Miscellaneous Citations</h1>
<p><a href="https://www.jstatsoft.org/article/view/v080i01">“https://www.jstatsoft.org/article/view/v080i01”</a></p>
<p><a href="https://bayesball.github.io/BOOK/bayesian-hierarchical-modeling.html">“https://bayesball.github.io/BOOK/bayesian-hierarchical-modeling.html”</a></p>
<p><a href="https://doi.org/10.1016/0370-2693(87)91197-X">“https://doi.org/10.1016/0370-2693(87)91197-X”</a></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Yes, I am well aware that there are massive philosophical differences in the view of probability and that they facilitate the methods, I am just not of the view that we need to necessarily agree with either in order to benefit from what each framework can deliver.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>lmao<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>verifying this goes a bit beyond the scope of this blog, but we can use Bayes rule to express the full conditional distribution into probability density functions: <span class="math inline">\(p(\alpha_j | \mathbf{y}, \mu_{\alpha}, \sigma_y, \sigma_{\alpha}) \propto p(\mathbf{y} | \alpha_j, \sigma_y) \times p(\alpha_j | \mu_{\alpha}, \sigma_{\alpha})\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://www.jstatsoft.org/article/view/v080i01, https://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf, https://www.sciencedirect.com/science/article/pii/037026938791197X?via%3Dihub<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://mc-stan.org/rstan/reference/Rhat.html<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The details for both effective sample size and <span class="math inline">\(\hat{R}\)</span> are not important for the scope of this tutorial.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>